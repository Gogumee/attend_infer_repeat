{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path as osp\n",
    "import tensorflow as tf\n",
    "\n",
    "from evaluation import make_fig, make_logger\n",
    "from experiment_tools import load, init_checkpoint, parse_flags, print_flags, set_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define flags\n",
    "flags = tf.flags\n",
    "\n",
    "flags.DEFINE_string('data_config', 'configs/static_mnist_data.py', '')\n",
    "flags.DEFINE_string('model_config', 'configs/imp_weighted_nvil.py', '')\n",
    "flags.DEFINE_string('results_dir', '../checkpoints', '')\n",
    "flags.DEFINE_string('run_name', 'test_run', '')\n",
    "\n",
    "flags.DEFINE_integer('batch_size', 64, '')\n",
    "\n",
    "flags.DEFINE_integer('summary_every', 1000, '')\n",
    "flags.DEFINE_integer('log_every', 5000, '')\n",
    "flags.DEFINE_integer('save_every', 5000, '')\n",
    "flags.DEFINE_integer('max_train_iter', int(3 * 1e5), '')\n",
    "flags.DEFINE_boolean('restart', False, '')\n",
    "\n",
    "flags.DEFINE_float('eval_size_fraction', .01, '')\n",
    "\n",
    "# # Parse flags\n",
    "# parse_flags()\n",
    "# F = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_flags(\n",
    "    log_every=500,\n",
    "    eval_size_fraction=0.01,\n",
    "    model_config='configs/vimco.py',\n",
    "    learning_rate=1e-5,\n",
    "    vimco_per_sample_control=True,\n",
    ")\n",
    "\n",
    "# Parse flags\n",
    "parse_flags()\n",
    "F = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading flags from configs/vimco.py\n",
      "loading flags from configs/static_mnist_data.py\n"
     ]
    }
   ],
   "source": [
    "# Prepare enviornment\n",
    "logdir = osp.join(F.results_dir, F.run_name)\n",
    "logdir, flags, restart_checkpoint = init_checkpoint(logdir, F.data_config, F.model_config, F.restart)\n",
    "checkpoint_name = osp.join(logdir, 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 'static_mnist_data' from configs/static_mnist_data.pyc\n",
      "Loading 'vimco' from configs/vimco.pyc\n",
      "iw_samples 5\n",
      "Flags:\n",
      "batch_size: 64\n",
      "data_config: configs/static_mnist_data.py\n",
      "eval_size_fraction: 0.01\n",
      "git_commit: 9ee473d9d43bdeb436ae9738d62a78b922bde5c6\n",
      "importance_resample: False\n",
      "learning_rate: 1e-05\n",
      "log_every: 500\n",
      "max_train_iter: 300000\n",
      "model_config: configs/vimco.py\n",
      "n_iw_samples: 5\n",
      "n_steps_per_image: 3\n",
      "restart: False\n",
      "results_dir: ../checkpoints\n",
      "run_name: test_run\n",
      "save_every: 5000\n",
      "step_bias: 1.0\n",
      "summary_every: 1000\n",
      "train_path: mnist_train.pickle\n",
      "transform_var_bias: -3.0\n",
      "use_r_imp_weight: True\n",
      "valid_path: mnist_validation.pickle\n",
      "vimco_per_sample_control: True\n"
     ]
    }
   ],
   "source": [
    "# Build the graph\n",
    "tf.reset_default_graph()\n",
    "data_dict = load(F.data_config, F.batch_size)\n",
    "air, train_step, global_step = load(F.model_config, img=data_dict.train_img, num=data_dict.train_num)\n",
    "\n",
    "print_flags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "if restart_checkpoint is not None:\n",
    "    print \"Restoring from '{}'\",format(restart_checkpoint)\n",
    "    saver.restore(sess, restart_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "all_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_logger: unable to log all expressions:\n",
      "\tSkipping baseline_loss\n",
      "\tSkipping nums_xe\n",
      "\tSkipping l2_loss\n"
     ]
    }
   ],
   "source": [
    "# Logging\n",
    "ax = data_dict['axes']['imgs']\n",
    "factor = F.eval_size_fraction\n",
    "train_batches, valid_batches = [int(data_dict[k]['imgs'].shape[ax] * factor) for k in ('train_data', 'valid_data')]\n",
    "log = make_logger(air, sess, summary_writer, data_dict.train_tensors,\n",
    "                  train_batches, data_dict.valid_tensors, valid_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388.31 425.072 62.6797 53.9437\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a, b = sess.run([air.nelbo_per_sample, air.baseline])\n",
    "print abs(a).mean(), abs(b).mean(), abs(a-b).mean(), (a-b).mean()\n",
    "# print np.concatenate((a, b), -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# o = sess.run(air.control)\n",
    "# # o = sess.run(air.biggest)\n",
    "# # o = sess.run(air.second_biggest)\n",
    "# # o = sess.run(air.all_but_one_average)\n",
    "# # o = sess.run(air.summed_exped_per_sample_elbo)\n",
    "# # o = sess.run(air.summed_exped_per_sample_elbo - air.exped_per_sample_elbo)\n",
    "# # o = sess.run(air.all_but_one_average - air.control)\n",
    "# #\n",
    "# print np.isnan(o).any(), o.min(), o.mean(), o.max()\n",
    "# print o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1) (64, 5)\n"
     ]
    }
   ],
   "source": [
    "print a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training at iter = 0\n",
      "Step 0, Data train kl_what = 1.4929, kl_shift = 8.4830, nelbo = -342.9016, num_step_acc = 0.2083, kl_num_steps = 14.6062, reinforce_loss = -371.0725, rec_loss = 355.9470, num_step = 1.6875, kl_scale = 8.4921, kl_div = 33.0742, kl_where = 16.9751, proxy_loss = -713.9741, eval time = 0.8359s\n",
      "Step 0, Data test kl_what = 0.6001, kl_shift = 7.1789, nelbo = -392.7419, num_step_acc = 0.1719, kl_num_steps = 14.5929, reinforce_loss = -389.8730, rec_loss = 174.4362, num_step = 1.4219, kl_scale = 7.0753, kl_div = 29.4472, kl_where = 14.2542, proxy_loss = -782.6150, eval time = 0.1787s\n",
      "\n",
      "Step 500, Data train kl_what = 1.0298, kl_shift = 4.1411, nelbo = -623.8975, num_step_acc = 0.3403, kl_num_steps = 15.5134, reinforce_loss = -77.7015, rec_loss = -144.3114, num_step = 0.7431, kl_scale = 4.2879, kl_div = 24.9721, kl_where = 8.4290, proxy_loss = -701.5990, eval time = 0.9346s\n",
      "Step 500, Data test kl_what = 1.3271, kl_shift = 5.8700, nelbo = -596.7224, num_step_acc = 0.3281, kl_num_steps = 15.4586, reinforce_loss = -128.5990, rec_loss = -27.2361, num_step = 1.0469, kl_scale = 6.0192, kl_div = 28.6750, kl_where = 11.8892, proxy_loss = -725.3214, eval time = 0.1201s\n",
      "\n",
      "Step 1000, Data train kl_what = 7.6290, kl_shift = 6.0526, nelbo = -657.8616, num_step_acc = 0.3750, kl_num_steps = 15.6880, reinforce_loss = -44.0627, rec_loss = -261.7890, num_step = 1.0087, kl_scale = 6.2500, kl_div = 35.6196, kl_where = 12.3026, proxy_loss = -701.9243, eval time = 0.7914s\n",
      "Step 1000, Data test kl_what = 7.7334, kl_shift = 6.7939, nelbo = -683.1925, num_step_acc = 0.3438, kl_num_steps = 15.7125, reinforce_loss = -39.8785, rec_loss = -267.1933, num_step = 1.0781, kl_scale = 6.8004, kl_div = 37.0402, kl_where = 13.5942, proxy_loss = -723.0710, eval time = 0.09054s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_itr = sess.run(global_step)\n",
    "print 'Starting training at iter = {}'.format(train_itr)\n",
    "\n",
    "if train_itr == 0:\n",
    "    log(0)\n",
    "\n",
    "while train_itr < F.max_train_iter:\n",
    "\n",
    "    train_itr, _ = sess.run([global_step, train_step])\n",
    "\n",
    "    if train_itr % F.summary_every == 0:\n",
    "        summaries = sess.run(all_summaries)\n",
    "        summary_writer.add_summary(summaries, train_itr)\n",
    "\n",
    "    if train_itr % F.log_every == 0:\n",
    "        log(train_itr)\n",
    "\n",
    "    if train_itr % F.save_every == 0:\n",
    "        saver.save(sess, checkpoint_name, global_step=train_itr)\n",
    "        make_fig(air, sess, logdir, train_itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make_fig(air, sess, n_samples=64) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
