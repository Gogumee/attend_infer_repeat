{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path as osp\n",
    "import tensorflow as tf\n",
    "\n",
    "from evaluation import make_fig, make_logger\n",
    "from experiment_tools import load, init_checkpoint, parse_flags, print_flags, set_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define flags\n",
    "flags = tf.flags\n",
    "\n",
    "flags.DEFINE_string('data_config', 'configs/static_mnist_data.py', '')\n",
    "flags.DEFINE_string('model_config', 'configs/imp_weighted_nvil.py', '')\n",
    "flags.DEFINE_string('results_dir', '../checkpoints', '')\n",
    "flags.DEFINE_string('run_name', 'test_run', '')\n",
    "\n",
    "flags.DEFINE_integer('batch_size', 64, '')\n",
    "\n",
    "flags.DEFINE_integer('summary_every', 1000, '')\n",
    "flags.DEFINE_integer('log_every', 5000, '')\n",
    "flags.DEFINE_integer('save_every', 5000, '')\n",
    "flags.DEFINE_integer('max_train_iter', int(3 * 1e5), '')\n",
    "flags.DEFINE_boolean('restart', False, '')\n",
    "\n",
    "flags.DEFINE_float('eval_size_fraction', .01, '')\n",
    "\n",
    "# Parse flags\n",
    "parse_flags()\n",
    "F = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_flags(\n",
    "    log_every=500,\n",
    "    eval_size_fraction=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare enviornment\n",
    "logdir = osp.join(F.results_dir, F.run_name)\n",
    "logdir, flags, restart_checkpoint = init_checkpoint(logdir, F.data_config, F.model_config, F.restart)\n",
    "checkpoint_name = osp.join(logdir, 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 'static_mnist_data' from configs/static_mnist_data.pyc\n",
      "Loading 'imp_weighted_nvil' from configs/imp_weighted_nvil.pyc\n",
      "iw_samples 5\n",
      "KL by sampling!\n",
      "KL by sampling!\n",
      "KL by sampling!\n",
      "KL by sampling!\n",
      "constructed baseline\n",
      "iw resampling!\n",
      "resampled [<tf.Tensor 'Gather:0' shape=(64,) dtype=float32>]\n",
      "iw resampling!\n",
      "resampled [<tf.Tensor 'loss_1/Gather:0' shape=(64,) dtype=float32>]\n",
      "iw resampling!\n",
      "resampled [<tf.Tensor 'loss_1/Gather_1:0' shape=(64,) dtype=float32>]\n",
      "iw resampling!\n",
      "resampled [<tf.Tensor 'loss_1/Gather_2:0' shape=(64,) dtype=float32>]\n",
      "iw resampling!\n",
      "resampled [<tf.Tensor 'loss_1/Gather_3:0' shape=(64,) dtype=float32>]\n",
      "iw resampling!\n",
      "resampled [<tf.Tensor 'loss_1/Gather_4:0' shape=(64,) dtype=float32>]\n",
      "iw resampling!\n",
      "resampled [<tf.Tensor 'loss_1/Gather_5:0' shape=(64,) dtype=float32>]\n",
      "AIR tensors:\n",
      "baseline (64, 1)\n",
      "baseline_loss ()\n",
      "canvas (320, 50, 50)\n",
      "elbo_importance_weights (64, 5)\n",
      "glimpse (320, 3, 20, 20)\n",
      "gt_num_steps (64,)\n",
      "kl_div ()\n",
      "kl_div_per_sample (320,)\n",
      "kl_num_steps ()\n",
      "kl_num_steps_per_sample (320,)\n",
      "kl_what ()\n",
      "kl_what_per_sample (320,)\n",
      "kl_where ()\n",
      "kl_where_per_sample (320,)\n",
      "negative_weighted_per_sample_elbo (64, 5)\n",
      "nelbo ()\n",
      "nelbo_per_sample (64, 1)\n",
      "num_step ()\n",
      "num_step_accuracy ()\n",
      "num_step_per_sample (320,)\n",
      "num_step_prior_prob (4,)\n",
      "num_steps_learning_signal (64, 5)\n",
      "obs (64, 50, 50)\n",
      "ordered_step_prob (320, 3)\n",
      "presence (320, 3, 1)\n",
      "presence_prob (320, 3, 1)\n",
      "proxy_loss ()\n",
      "rec_loss ()\n",
      "rec_loss_per_sample (320,)\n",
      "reinforce_loss ()\n",
      "steps_prior_success_prob ()\n",
      "used_obs (320, 50, 50)\n",
      "what (320, 3, 50)\n",
      "what_loc (320, 3, 50)\n",
      "what_scale (320, 3, 50)\n",
      "where (320, 3, 4)\n",
      "where_loc (320, 3, 4)\n",
      "where_scale (320, 3, 4)\n",
      "Flags:\n",
      "batch_size: 64\n",
      "data_config: configs/static_mnist_data.py\n",
      "eval_size_fraction: 0.01\n",
      "importance_resample: False\n",
      "learning_rate: 1e-05\n",
      "log_every: 500\n",
      "max_train_iter: 300000\n",
      "model_config: configs/imp_weighted_nvil.py\n",
      "n_iw_samples: 5\n",
      "n_steps_per_image: 3\n",
      "restart: False\n",
      "results_dir: ../checkpoints\n",
      "run_name: test_run\n",
      "save_every: 5000\n",
      "step_bias: 1.0\n",
      "summary_every: 1000\n",
      "train_path: mnist_train.pickle\n",
      "transform_var_bias: -3.0\n",
      "use_r_imp_weight: True\n",
      "valid_path: mnist_validation.pickle\n"
     ]
    }
   ],
   "source": [
    "# Build the graph\n",
    "tf.reset_default_graph()\n",
    "data_dict = load(F.data_config, F.batch_size)\n",
    "air, train_step, global_step = load(F.model_config, img=data_dict.train_img, num=data_dict.train_num)\n",
    "\n",
    "print_flags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "if restart_checkpoint is not None:\n",
    "    print \"Restoring from '{}'\",format(restart_checkpoint)\n",
    "    saver.restore(sess, restart_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "all_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_logger: unable to log all expressions:\n",
      "\tSkipping nums_xe\n",
      "\tSkipping l2_loss\n"
     ]
    }
   ],
   "source": [
    "# Logging\n",
    "ax = data_dict['axes']['imgs']\n",
    "factor = F.eval_size_fraction\n",
    "train_batches, valid_batches = [int(data_dict[k]['imgs'].shape[ax] * factor) for k in ('train_data', 'valid_data')]\n",
    "log = make_logger(air, sess, summary_writer, data_dict.train_tensors,\n",
    "                  train_batches, data_dict.valid_tensors, valid_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training at iter = 0\n",
      "Step 0, Data train baseline_loss = 119003.3220, kl_what = 0.1543, nelbo = -344.0491, num_step_acc = 0.3264, kl_num_steps = 14.5199, reinforce_loss = 2313.2861, rec_loss = -366.3901, num_step = 0.6111, kl_div = 20.7757, kl_where = 6.1384, proxy_loss = 1969.2370, eval time = 0.9988s\n",
      "Step 0, Data test baseline_loss = 109291.7031, kl_what = 0.2041, nelbo = -319.3570, num_step_acc = 0.4062, kl_num_steps = 14.4977, reinforce_loss = 2098.5679, rec_loss = -342.6302, num_step = 0.6875, kl_div = 21.7059, kl_where = 7.0041, proxy_loss = 1779.2109, eval time = 0.2062s\n",
      "\n",
      "Step 500, Data train baseline_loss = 12288.2697, kl_what = 0.1719, nelbo = -631.5133, num_step_acc = 0.3160, kl_num_steps = 15.0484, reinforce_loss = -152.8551, rec_loss = -651.6534, num_step = 0.4253, kl_div = 18.7251, kl_where = 3.5292, proxy_loss = -784.3684, eval time = 0.9819s\n",
      "Step 500, Data test baseline_loss = 16116.3252, kl_what = 1.0604, nelbo = -595.7603, num_step_acc = 0.2344, kl_num_steps = 15.0188, reinforce_loss = -350.4907, rec_loss = -618.0233, num_step = 0.5469, kl_div = 20.8255, kl_where = 4.7273, proxy_loss = -946.2510, eval time = 0.1364s\n",
      "\n",
      "Step 1000, Data train baseline_loss = 12159.5372, kl_what = 0.0665, nelbo = -631.2396, num_step_acc = 0.2951, kl_num_steps = 15.2590, reinforce_loss = -135.6917, rec_loss = -649.4781, num_step = 0.2778, kl_div = 16.9035, kl_where = 1.5933, proxy_loss = -766.9312, eval time = 0.7679s\n",
      "Step 1000, Data test baseline_loss = 2393.5820, kl_what = 0.0068, nelbo = -682.4697, num_step_acc = 0.3281, kl_num_steps = 15.3652, reinforce_loss = 167.9957, rec_loss = -700.1113, num_step = 0.1719, kl_div = 16.3192, kl_where = 1.0640, proxy_loss = -514.4740, eval time = 0.08552s\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8df9ae56f6a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mtrain_itr\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_train_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_itr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrain_itr\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adam/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adam/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adam/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/adam/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adam/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_itr = sess.run(global_step)\n",
    "print 'Starting training at iter = {}'.format(train_itr)\n",
    "\n",
    "if train_itr == 0:\n",
    "    log(0)\n",
    "\n",
    "while train_itr < F.max_train_iter:\n",
    "\n",
    "    train_itr, _ = sess.run([global_step, train_step])\n",
    "\n",
    "    if train_itr % F.summary_every == 0:\n",
    "        summaries = sess.run(all_summaries)\n",
    "        summary_writer.add_summary(summaries, train_itr)\n",
    "\n",
    "    if train_itr % F.log_every == 0:\n",
    "        log(train_itr)\n",
    "\n",
    "    if train_itr % F.save_every == 0:\n",
    "        saver.save(sess, checkpoint_name, global_step=train_itr)\n",
    "        make_fig(air, sess, logdir, train_itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "make_fig(air, sess, n_samples=64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s = air.iw_distrib.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 2, 4, 3, 4, 4, 1, 2, 1, 2, 3, 2, 0, 2, 1, 3, 0, 3, 3, 0, 1, 0,\n",
       "       1, 2, 3, 0, 1, 0, 2, 1, 0, 4, 4, 3, 4, 0, 1, 0, 1, 4, 0, 1, 0, 1, 1,\n",
       "       4, 2, 4, 1, 4, 1, 0, 3, 1, 2, 0, 0, 0, 3, 1, 2, 0, 3], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'distributions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3a0bebc33a14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miw_distrib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'distributions'"
     ]
    }
   ],
   "source": [
    "d = air.iw_distrib\n",
    "d.logits\n",
    "tf.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(air.iw_distrib.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
