{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path as osp\n",
    "import tensorflow as tf\n",
    "\n",
    "from evaluation import make_fig, make_logger\n",
    "from experiment_tools import load, init_checkpoint, parse_flags, print_flags, set_flags_if_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define flags\n",
    "flags = tf.flags\n",
    "\n",
    "flags.DEFINE_string('data_config', 'configs/static_mnist_data.py', '')\n",
    "flags.DEFINE_string('model_config', 'configs/imp_weighted_nvil.py', '')\n",
    "flags.DEFINE_string('results_dir', '../checkpoints', '')\n",
    "flags.DEFINE_string('run_name', 'test_run', '')\n",
    "\n",
    "flags.DEFINE_integer('batch_size', 64, '')\n",
    "\n",
    "flags.DEFINE_integer('summary_every', 1000, '')\n",
    "flags.DEFINE_integer('log_every', 5000, '')\n",
    "flags.DEFINE_integer('save_every', 5000, '')\n",
    "flags.DEFINE_integer('max_train_iter', int(3 * 1e5), '')\n",
    "flags.DEFINE_boolean('restart', False, '')\n",
    "\n",
    "flags.DEFINE_float('eval_size_fraction', .01, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_flags_if_notebook(\n",
    "    log_every=500,\n",
    "    eval_size_fraction=0.01,\n",
    "    model_config='configs/vimco.py',\n",
    "    learning_rate=1e-5,\n",
    "    vimco_per_sample_control=True\n",
    ")\n",
    "\n",
    "# Parse flags\n",
    "parse_flags()\n",
    "F = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare enviornment\n",
    "logdir = osp.join(F.results_dir, F.run_name)\n",
    "logdir, flags, restart_checkpoint = init_checkpoint(logdir, F.data_config, F.model_config, F.restart)\n",
    "checkpoint_name = osp.join(logdir, 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "tf.reset_default_graph()\n",
    "data_dict = load(F.data_config, F.batch_size)\n",
    "air, train_step, global_step = load(F.model_config, img=data_dict.train_img, num=data_dict.train_num)\n",
    "\n",
    "print_flags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "if restart_checkpoint is not None:\n",
    "    print \"Restoring from '{}'\",format(restart_checkpoint)\n",
    "    saver.restore(sess, restart_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "all_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Logging\n",
    "ax = data_dict['axes']['imgs']\n",
    "factor = F.eval_size_fraction\n",
    "train_batches, valid_batches = [int(data_dict[k]['imgs'].shape[ax] * factor) for k in ('train_data', 'valid_data')]\n",
    "log = make_logger(air, sess, summary_writer, data_dict.train_tensors,\n",
    "                  train_batches, data_dict.valid_tensors, valid_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a, b = sess.run([air.nelbo_per_sample, air.baseline])\n",
    "print abs(a).mean(), abs(b).mean(), abs(a-b).mean(), (a-b).mean()\n",
    "# print np.concatenate((a, b), -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# o = sess.run(air.control)\n",
    "# # o = sess.run(air.biggest)\n",
    "# # o = sess.run(air.second_biggest)\n",
    "# # o = sess.run(air.all_but_one_average)\n",
    "# # o = sess.run(air.summed_exped_per_sample_elbo)\n",
    "# # o = sess.run(air.summed_exped_per_sample_elbo - air.exped_per_sample_elbo)\n",
    "# # o = sess.run(air.all_but_one_average - air.control)\n",
    "# #\n",
    "# print np.isnan(o).any(), o.min(), o.mean(), o.max()\n",
    "# print o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_itr = sess.run(global_step)\n",
    "print 'Starting training at iter = {}'.format(train_itr)\n",
    "\n",
    "if train_itr == 0:\n",
    "    log(0)\n",
    "\n",
    "while train_itr < F.max_train_iter:\n",
    "\n",
    "    train_itr, _ = sess.run([global_step, train_step])\n",
    "\n",
    "    if train_itr % F.summary_every == 0:\n",
    "        summaries = sess.run(all_summaries)\n",
    "        summary_writer.add_summary(summaries, train_itr)\n",
    "\n",
    "    if train_itr % F.log_every == 0:\n",
    "        log(train_itr)\n",
    "\n",
    "    if train_itr % F.save_every == 0:\n",
    "        saver.save(sess, checkpoint_name, global_step=train_itr)\n",
    "        make_fig(air, sess, logdir, train_itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make_fig(air, sess, n_samples=64) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
